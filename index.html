<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Design2GarmentCode: Turning Design Concepts to Tangible Garments Through Program Synthesis">
  <meta property="og:title" content="Design2GarmentCode"/>
  <meta property="og:description" content="Turning Design Concepts to Tangible Garments Through Program Synthesis"/>
  <meta property="og:url" content="https://style3d.github.io/design2garmentcode/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Design2GarmentCode">
  <meta name="twitter:description" content="Turning Design Concepts to Tangible Garments Through Program Synthesis">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="garment generation, sewing patterns">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Design2GarmentCode</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

    <style>
        .video-wrapper {
    position: relative;
    text-align: center;
    width: 100%;
    max-width: 1200px; /* 增大最大宽度 */
    margin: 0 auto;
        }

      .video-container {
          display: none;
          width: 100%;
      }

      .video-container.is-active {
          display: block;
      }

      .video-container video {
          width: 100%; /* 让视频填满容器 */
          height: auto;
          max-height: 600px; /* 限制最大高度，避免太大 */
          display: block;
          object-fit: cover; /* 适应不同分辨率 */
      }

      .nav-button {
          position: absolute;
          top: 50%;
          transform: translateY(-50%);
          background-color: rgba(0, 0, 0, 0.1);
          color: white;
          border: none;
          padding: 15px 20px; /* 增大按钮大小 */
          cursor: pointer;
          z-index: 1000;
          font-size: 32px; /* 让箭头更大 */
          border-radius: 5px;
          opacity: 0.7; /* 整体透明度 */
          transition: opacity 0.3s ease-in-out;
      }

      .prev-button {
          left: -60px; /* 确保箭头在视频外部 */
      }

      .next-button {
          right: -60px; /* 确保箭头在视频外部 */
      }

.pagination-dots {
    display: flex;
    justify-content: center;
    margin-top: 10px;
}

.dot {
    width: 12px;
    height: 12px;
    margin: 0 5px;
    background-color: #bbb;
    border-radius: 50%;
    display: inline-block;
    transition: background-color 0.3s;
    cursor: pointer;
}

.dot.active {
    background-color: #333; /* 当前选中的黑点变黑 */
}
    </style>

</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Design2GarmentCode: <br> 
              <small><small><small><small>
                Turning Design Concepts to Tangible Garments Through Program Synthesis
              </small></small></small></small>
            </h1>
              
            2025 Computer Vision and Pattern Recognition (CVPR 2025)
            
              <div class="is-size-6 publication-authors">
              <br>
              <!-- Paper authors -->
              <span class="author-block"><a href="">Feng Zhou</a><sup>1,2</sup>,&nbsp;</span>
              <span class="author-block"><a href="https://walnut-ree.github.io/">Ruiyang Liu</a><sup>2,&dagger;</sup>,&nbsp;</span>
              <span class="author-block"><a href="">Chen Liu</a><sup>2</sup>,&nbsp;</span>
              <span class="author-block"><a href="">Gaofeng He</a><sup>2</sup>,&nbsp;</span>
              <span class="author-block"><a href="https://dirtyharrylyl.github.io/">Yong-Lu Li</a><sup>3</sup>,&nbsp;</span>
              <span class="author-block"><a href="http://www.cad.zju.edu.cn/home/jin/">Xiaogang Jin</a><sup>4</sup>,&nbsp;</span>
              <span class="author-block"><a href="https://wanghmin.github.io/">Huamin Wang</a><sup>2</sup>,</span>
              </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Zhejiang Sci-Tech University,&nbsp;&nbsp;&nbsp;&nbsp;</span>
                    <span class="author-block"><sup>2</sup>Style3D Research,&nbsp;&nbsp;&nbsp;&nbsp;</span>
                    <span class="author-block"><sup>3</sup>Shanghai Jiao Tong University,&nbsp;&nbsp;&nbsp;&nbsp;</span>
                    <span class="author-block"><sup>4</sup>Zhejiang University</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="http://arxiv.org/abs/2412.08603" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Style3D/SXDGarmentCode" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code & Data</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href=""  target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-democrat	"></i>
                  </span>
                  <span>Playground</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-vcentered">
        <div class="column is-half">
          <figure class="image">
            <img src="static/images/neural_symbolic-teaser.png" alt="Teaser Image">
          </figure>
        </div>
        <div class="column is-half">
          <h2 class="content has-text-left">
              Design2GarmentCode is a motility-agnostic sewing pattern generation framework that leverages fine-tuned Large Multimodal Models to generate parametric pattern-making programs from multi-modal design concepts.
              <br><br>
              Compared with previous methods that directly synthesize vector-quantized patterns, synthesizing directly parametric pattern-making programs allows Design2GarmentCode to generate dedicated, structurally correct patterns from multi-modal design inputs with minimal computational and data requirements.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!--  <section class="hero teaser">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <br>-->
<!--&lt;!&ndash;      <h2 class="title is-3">Applications</h2>&ndash;&gt;-->
<!--      <video poster="" id="dance_video" autoplay  muted loop height="20%">-->
<!--        &lt;!&ndash; Your video here &ndash;&gt;-->
<!--        <source src="static/videos/video_show.mp4"-->
<!--        type="video/mp4">-->
<!--      </video>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
    <section class="section hero teaser">
        <div class="container is-max-desktop">
            <div class="video-wrapper">
                <button class="nav-button prev-button">❮</button>
                <button class="nav-button next-button">❯</button>

                <div id="video1" class="video-container is-active">
                    <video autoplay controls muted loop playsinline>
                        <source src="./static/videos/index.mp4" type="video/mp4">
                    </video>
                </div>
                <div id="video2" class="video-container">
                    <video autoplay controls muted loop playsinline>
                        <source src="./static/videos/sketch.mp4" type="video/mp4">
                    </video>
                </div>
                <div id="video3" class="video-container">
                    <video autoplay controls muted loop playsinline>
                        <source src="./static/videos/text.mp4" type="video/mp4">
                    </video>
                </div>
                <div id="video4" class="video-container">
                    <video autoplay controls muted loop playsinline>
                        <source src="./static/videos/picture.mp4" type="video/mp4">
                    </video>
                </div>
                <div class="pagination-dots"></div>
            </div>
        </div>
    </section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Sewing patterns, the essential blueprints for fabric cutting and tailoring, act as a crucial bridge between design concepts and producible garments.
            However, existing uni-modal sewing pattern generation models struggle to effectively encode complex design concepts with a multi-modal nature and correlate them with vectorized sewing patterns that possess precise geometric structures and intricate sewing relations.
            In this work, we propose a novel sewing pattern generation approach <b>Design2GarmentCode</b> based on Large Multimodal Models (LMMs), to generate parametric pattern-making programs from multi-modal design concepts.
            LMM offers an intuitive interface for interpreting diverse design inputs, while pattern-making programs could serve as well-structured and semantically meaningful representations of sewing patterns, and act as a robust bridge connecting the cross-domain pattern-making knowledge embedded in LMMs with vectorized sewing patterns.
            Experimental results demonstrate that our method can flexibly handle various complex design expressions such as images, textual descriptions, designer sketches, or their combinations, and convert them into size-precise sewing patterns with correct stitches. Compared to previous methods, our approach significantly enhances training efficiency, generation quality, and authoring flexibility.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Method -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br>
      <h2 class="title is-3">Method</h2>
      <figure class="image">
        <img src="static/images/neural_symbolic-pipeline.png" alt="Pipeline Overview">
      </figure>
      <h2 class="content">
          <br>
          <b>Overview of Dress2GarmentCode.</b>
          <small>
          (1) <b>Program Learning</b>: we finetune the <i>DSL Generation Agent (DSL-GA)</i> using GarmentCode example programs, teaching it the GarmentCode grammar and the semantics of each design parameter.
          (2) <b>Prompt Synthesis</b>: the DSL-GA generates prompts for the <i>Multi-Modal Understanding Agent (MMUA)</i> to interpret and extract relevant design features from the input (3).
          (4) <b>Program Synthesis</b>: based on the MMUA's responses, the DSL-GA synthesizes GarmentCode-compliant design configurations and garment programs, which are then executed by the GarmentCode engine to produce sewing patterns and simulated garments (5).
          To enhance robustness, we incorporate two validation loops: during program synthesis, we employ rule-based validations (7) to ensure the MMUA's outputs are sufficient for generating complete and valid garment programs and design parameters; after the initial generation, the MMUA compares the generated design with the input and suggests modifications to minimize discrepancies.
        </small>
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->


<!-- Method -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br>
      <h2 class="title is-3">Text Guided Generation</h2>
      <h2 class="content">
        <br>
        <b>Quality Comparison on Text-Guided Sewing Pattern Generation.</b>
        <small>
          For each design, we present the generated pattern using our method (left) alongside <a href="https://ihe-kaii.github.io/DressCode/">DressCode</a> (right), including front and back renderings of the draped garment.
          We highlight design elements accurately captured by our method but missed by <a href="https://ihe-kaii.github.io/DressCode/">DressCode</a> use red color in the input prompt.
        </small>
      </h2>
      <figure class="image">
        <img src="static/images/result_text.png" alt="Text Guided Generation Results">
      </figure>
    </div>
  </div>
</section>
<!-- End teaser video -->


<!-- Method -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br>
      <h2 class="title is-3">Image Guided Generation</h2>
      <h2 class="content">
        <br>
        <b>Quality Comparison on Image-Guided Sewing Pattern Generation.</b>
        <small>
          We compare our method with <a href="https://sewformer.github.io/">SewFormer</a> on Internet-collected fashion photographs (left),
          and AI-generated design images without human models (right).
          The results indicate that our method successfully captures design details from diverse styles, producing sewing patterns that accurately reflect neckline (a, d), cuffs (a, e, g), darts (c, d), and asymmetry (f).
          In contrast, <a href="https://sewformer.github.io/">SewFormer</a>'s results exhibit several issues, including incorrect necklines (a, d), missing components (b, g), misplaced or imaginary stitches (d, e), and extraneous pattern pieces (h).
          Additionally, since <a href="https://sewformer.github.io/">SewFormer</a>’s pattern generation does not account for body shape, garments like skirts and pants frequently appear oversized around the waist, causing them to sag when draped.
        </small>
      </h2>
      <figure class="image">
        <img src="static/images/result_image.png" alt="Image Guided Generation Results">
      </figure>

    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Method -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br>
      <h2 class="title is-3">Sketch Guided Generation</h2>
      <figure class="image">
        <img src="static/images/result_sketch.png" alt="Sketch Guided Generation Results">
      </figure>
      <h2 class="content">
          <br>
          <b>Examples of sketch-based sewing pattern generation. </b>
          <small>
            Our method was able to generate high-quality sewing patterns from design sketches under various styles and could integrate seamlessly with industrial fashion design software for
            (a) pattern editing, i.e. sleeve panels in red boxes are merged from separate front/back sleeve panels;
            and (b) avatar posture and fabric material editing.
          </small>
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Method -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br>
      <h2 class="title is-3">Generating New Garment Components</h2>
      <figure class="image">
        <img src="static/images/neural_symbolic-app_new_code.png" alt="Generating New Garment Components">
      </figure>
      <h2 class="content">
          <br>
          <b>Examples of sketch-based sewing pattern generation. </b>
          <small>
            A major challenge in traditional parametric pattern-making is the need to abstract symbolic programs for new sewing patterns,
            which demands both advanced programming skills and pattern-making expertise. Design2GarmentCode addresses this by correlating GarmentCode grammar with LMMs' embedded pattern-making knowledge,
            enabling the automatic creation of new garment components. Here we demonstrate the code for a layered-skirt component generated by our DSL-GA and simulated 3D garments under various design parameters.
          </small>
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->


<!-- Method -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br>
      <h2 class="title is-3">Applications</h2>
      <video poster="" id="tree" autoplay controls muted loop height=" 20%">
        <!-- Your video here -->
        <source src="static/videos/d2g_demo.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser video -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre>
        <code>
          @article{zhou2024design2garmentcode,
            title={Design2GarmentCode: Turning Design Concepts to Tangible Garments Through Program Synthesis},
            author={Zhou, Feng and Liu, Ruiyang and Liu, Chen and He, Gaofeng and Li, Yong-Lu and Jin, Xiaogang and Wang, Huamin},
            journal={arXiv preprint arXiv:2412.08603},
            year={2024}
          }
        </code>
      </pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>


<script>
    document.addEventListener('DOMContentLoaded', function () {
        var videos = document.querySelectorAll(".video-container");
        var prevButton = document.querySelector(".prev-button");
        var nextButton = document.querySelector(".next-button");
        var paginationDots = document.querySelector(".pagination-dots");
        var currentIndex = 0;

        // 生成黑点
        videos.forEach((_, index) => {
            var dot = document.createElement("span");
            dot.classList.add("dot");
            if (index === 0) dot.classList.add("active"); // 初始选中第一个
            dot.setAttribute("data-index", index);
            dot.addEventListener("click", function () {
                currentIndex = parseInt(this.getAttribute("data-index"));
                showVideo(currentIndex);
            });
            paginationDots.appendChild(dot);
        });

        function showVideo(index) {
            if (index < 0 || index >= videos.length) return;

            videos.forEach((video, i) => {
                video.classList.toggle("is-active", i === index);
                video.querySelector("video").pause();
            });

            videos[index].querySelector("video").play();

            // 更新黑点
            document.querySelectorAll(".dot").forEach((dot, i) => {
                dot.classList.toggle("active", i === index);
            });
        }

        prevButton.addEventListener("click", function () {
            currentIndex = (currentIndex === 0) ? videos.length - 1 : currentIndex - 1;
            showVideo(currentIndex);
        });

        nextButton.addEventListener("click", function () {
            currentIndex = (currentIndex === videos.length - 1) ? 0 : currentIndex + 1;
            showVideo(currentIndex);
        });

        showVideo(currentIndex);
    });
</script>



  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
